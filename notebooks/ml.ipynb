{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# linear Regression Example and implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys\n",
    "from ml_implement.general_utils.plotmatlabfun import plot_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path.cwd().parent \n",
    "base_dir = Path(base_dir)\n",
    "print(f\"Base directory: {base_dir}\")\n",
    "from ml_implement.general_utils.data_read import DataReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filename = 'realest.csv'\n",
    "filepath = base_dir/'data'/'Linear_Regression'/'chicago_houseprice'\n",
    "# reader = DataReader(filepath, filename)\n",
    "# reader = DataReader(filepath=filepath, filename=filename, split=True, df_want=True, target_column=\"Price\")\n",
    "reader = DataReader(filepath=filepath, filename=filename, split=True, df_want=True, target_column=None)\n",
    "res = reader.run() # result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile = r\"/home/mrafiku/AI_learning/machine-learning-basics/data/Linear_Regression/placementdata/placement.csv\"\n",
    "datafile = Path(datafile)\n",
    "df = pd.read_csv(datafile)\n",
    "print(df.head())\n",
    "# print(df.info())\n",
    "# print(df.describe())\n",
    "plot_data(df,Xaxis=df.iloc[:,0],Yaxis=df.iloc[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV files often include:\n",
    "# Leading/trailing spaces\n",
    "# Hidden characters\n",
    "# -  Uppercase/lowercase mismatches\n",
    "# -  UTF-8 BOM characters\n",
    "# -  Inconsistent headers\n",
    "# Even if the DataFrame looks correct, the internal label might be different.\n",
    "\n",
    "print(df.columns.tolist())\n",
    "# print(df.columns)\n",
    "# Fix: normalize column names\n",
    "df.columns = df.columns.str.strip().str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df['cgpa'],df['package'])\n",
    "plt.xlabel('CGPA')\n",
    "plt.ylabel('Package')\n",
    "plt.title('CGPA vs package')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape # (n_rows, n_columns)\n",
    "df.size  # total number of elements in the DataFrame\n",
    "df.ndim  # number of dimensions (axes) of the DataFrame\n",
    "print(f\"DataFrame Shape: {df.shape} | Size: {df.size} | Dimensions: {df.ndim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = df[\"cgpa\"]\n",
    "# X.shape\n",
    "# X.ndim\n",
    "# X = np.array(X).reshape(-1,1)\n",
    "# # X = X.reshape(-1,1)\n",
    "# X.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,0].values\n",
    "y = df.iloc[:,1].values \n",
    "print(f\"X shape: {X.shape} | type ox X: {type(X)}| y shape: {y.shape} | type of y: {type(y)}\")\n",
    "\n",
    "if X.ndim == 1:\n",
    "    X = X.reshape(-1,1)\n",
    "if y.ndim == 1:\n",
    "    y = y.reshape(-1,1)\n",
    "print(f\"After Reshaping: X shape: {X.shape}, y shape: {y.shape} \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "- df.describe()\n",
    "- df.columns.tolist()\n",
    "- print(f\" {X} \\n {y}\")\n",
    "- print(f\"X shape: {X.shape}, y shape: {y.shape} and \\n X type: {type(X)} | y type: {type(y)} \")  \n",
    "#--> These are the output of the print statement \n",
    "- those clearly shows: That shapes of X,y is one D array, so for the Sklearn we need to reshape it to 2D array : for fit and predict methods. \n",
    "- X shape: (200,), y shape: (200,) and \n",
    "- X type: <class 'numpy.ndarray'> | y type: <class 'numpy.ndarray'>\n",
    "#--> Reshaping the X to 2D array in th ebelow next cell: ---> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \" this is function to fit Linear Regression model and return X_train,X_test,y_train, y_test,model\"\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def LR_modelfit_predict_Result(X, y,test_size = None):\n",
    "    \"\"\"\n",
    "    Docstring for fit_LR_model_predict_Result is return result: X_train,X_test,y_train, y_test,model\n",
    "    :param X: X is numpy array of shape (n_samples, n_features)\n",
    "    :param y: y is numpy array of shape (n_samples, )\n",
    "    :param test_size: test_size is None,bydefault is 0.2 if None, 0 if 0, else float value between 0 and 1\n",
    "    :return: X_train,X_test,y_train, y_test,model\n",
    "    \"\"\"\n",
    "    if X.ndim == 1:\n",
    "        X = X.reshape(-1,1)\n",
    "    if y.ndim == 1:\n",
    "        y = y.reshape(-1,1)\n",
    "    if test_size == 0:\n",
    "        print(\"using all data for training as test_size is 0\")\n",
    "        X_train, y_train = X,y\n",
    "    if test_size is None:\n",
    "        test_size = 0.2\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "    if test_size is not None and test_size !=0:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "\n",
    "    print(f\"X_train shape: {X_train.shape}, X_test shape: {X_test.shape} , y_train shape: {y_train.shape}, y_test shape: {y_test.shape}\")\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    # y_pred = model.predict(X_test)\n",
    "    return X_train,X_test,y_train, y_test,model\n",
    "    # print(f\"input_CGPA: {X_test[0,0]}, PREDICTED_PACKAGE: {y_pred[0,0]} and ACTUAL PACKAGE: {y_test[0,0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this a module to import the metric of the linear regression from scratch\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score\n",
    "X_train,X_test,y_train, y_test,model = LR_modelfit_predict_Result(X, y,test_size=0.2)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test,y_pred)\n",
    "mse = mean_squared_error(y_test,y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test,y_pred)\n",
    "print(f\"MAE: {mae}, MSE: {mse}, RMSE: {rmse}, R2_score: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn.datasets..make_regression() --> used to generate the regression datasets provided by the scikit-learn.\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "X,y =  make_regression(n_samples = 100,n_features=2,n_informative=2,n_targets=1,noise=0.1, random_state=42)\n",
    "# print(f\"{X} \\n {y}\")\n",
    "x1 = X[:,0]\n",
    "x2 = X[:,1]\n",
    "# plt.scatter(x1,x2,y)\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "sc = ax.scatter(x1, x2, y, c=y, cmap='viridis', s=40, alpha=0.8)\n",
    "\n",
    "\n",
    "\n",
    "ax.set_xlabel('x1')\n",
    "ax.set_ylabel('x2')\n",
    "ax.set_zlabel('y')\n",
    "ax.set_title('3D scatter: x1, x2 vs y')\n",
    "fig.colorbar(sc, ax=ax, shrink=0.5, label='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code for linear regresssion model from the basics without using sklearn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionMine():\n",
    "    def __init__(self):\n",
    "\n",
    "        pass\n",
    "    def fit(self,X,y):\n",
    "        n = X.shape[0]  # number of training examples.\n",
    "        d = X.shape[1]  # numbers of features\n",
    "        \n",
    "        X = np.hstack((np.ones((n,1)),X))  # adding bias term to the feature matrix. \n",
    "        A = (X.T)@X\n",
    "        A_inv = np.linalg.inv(A)\n",
    "        B = (X.T)@y\n",
    "        Theta = A_inv @ B   # parameters of the model.\n",
    "        self.theta = Theta\n",
    "        return Theta\n",
    "    \n",
    "    def predict(self,X):\n",
    "        n = X.shape[0]\n",
    "        X = np.hstack((np.ones((n,1)),X))  # adding bias term to the feature matrix. \n",
    "        y_pred = X @ self.theta\n",
    "        self.y_pred = y_pred\n",
    "        return y_pred\n",
    "    \n",
    "    def MAE(self,y_true,y_pred):\n",
    "        n_test = y_true.shape[0]\n",
    "        mae = np.abs(np.sum(y_true - y_pred))/n_test\n",
    "        return mae\n",
    "    \n",
    "    def MSE(self,y_true,y_pred):\n",
    "        n_test = y_true.shape[0]\n",
    "        mse = np.sum((y_true - y_pred)**2)/n_test\n",
    "        return mse\n",
    "    def RMSE(self,y_true,y_pred):\n",
    "        mse = self.MSE(y_true,y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        return rmse\n",
    "    def R2_score(self,y_true,y_pred):\n",
    "        ss_total = np.sum((y_true - np.mean(y_true))**2)\n",
    "        ss_residual = np.sum((y_true - y_pred)**2)\n",
    "        r2 = 1 - (ss_residual/ss_total)\n",
    "        return r2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train,X_test,y_train,y_test= train_test_split(X,y,test_size =0.2,random_state = 42)\n",
    "# print(f\"X_train shape: {X_train.shape}, X_test shape: {X_test.shape} , y_train shape: {y_train.shape}, y_test shape: {y_test.shape}\")\n",
    "mlr = LinearRegressionMine()\n",
    "mlr.fit(X_train,y_train)\n",
    "y1_pred = mlr.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"input_CGPA: {X_test[0,0]}, PREDICTED_PACKAGE: {y1_pred[0,0]} and ACTUAL PACKAGE: {y_test[0,0]}\")\n",
    "# input_CGPA: 6.63, PREDICTED_PACKAGE: 2.7803134765595168 and ACTUAL PACKAGE: 2.79\n",
    "ame = mlr.MAE(y_test,y1_pred)\n",
    "mse = mlr.MSE(y_test,y1_pred)\n",
    "rmse = mlr.RMSE(y_test,y1_pred)\n",
    "r2 = mlr.R2_score(y_test,y1_pred)   \n",
    "print(f\"AME: {ame}, MSE: {mse}, RMSE: {rmse}, R2_score: {r2}\")\n",
    "# MAE: 0.23150985393278373, MSE: 0.08417638361329656, RMSE: 0.2901316659954521, R2_score: 0.7730984312051673\n",
    "\n",
    "# here we can see the results are comparable with the sklearn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {
    "keep_output": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Initialize the figure and axes\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 4))  # 1 row, 2 columns\n",
    "\n",
    "# --- FIRST SUBPLOT (Index 0) ---\n",
    "ax[0].scatter(X_test, y_test, color='blue', label='Actual', alpha=0.6)\n",
    "ax[0].plot(X_test, y_pred, '-.r', label='Predicted - Sklearn')\n",
    "ax[0].set_xlabel('CGPA')\n",
    "ax[0].set_ylabel('Package')\n",
    "ax[0].set_title('Sklearn Model')\n",
    "ax[0].grid(True)  # Use ax[0].grid() instead of plt.grid()\n",
    "ax[0].legend()\n",
    "\n",
    "# --- SECOND SUBPLOT (Index 1) ---\n",
    "ax[1].scatter(X_test, y_test, color='blue', label='Actual', alpha=0.6)\n",
    "ax[1].plot(X_test, y1_pred, '-.y', label='Predicted - MineLR')\n",
    "ax[1].set_xlabel('CGPA')\n",
    "ax[1].set_ylabel('Package')\n",
    "ax[1].set_title('My_LinearRegression_Code Model')\n",
    "ax[1].grid(True)  # Use ax[1].grid() instead of plt.grid()\n",
    "ax[1].legend()\n",
    "\n",
    "# Final adjustments\n",
    "plt.tight_layout() \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent: \n",
    "# bn+1  = bn - α * ∇J / @ b = bn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "X,y  = make_regression(n_samples=7, n_features=1, n_targets=1, noise=50, random_state=42)\n",
    "# from sklearn.Linear_model import LinearRegression\n",
    "\n",
    "model  = LinearRegression()\n",
    "model.fit(X,y)\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.scatter(X,y, color='blue', label='Data Points')\n",
    "plt.plot(X, y_pred, color='red', label='Linear Regression Line')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Linear Regression Fit')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "# model.intercept_  # b0  --> bias term\n",
    "model.coef_       # b1  --> weight term\n",
    "m = model.coef_[0]  # slope (m)\n",
    "print(f\"Slope (m): {m} and shape : {model.coef_.shape} \")\n",
    " \n",
    "learning_rate = 0.01  # learning rate\n",
    "n = X.shape[0]  # number of training examples\n",
    "b = 0\n",
    "epochs = 25\n",
    "        \n",
    "plt.figure(figsize=(8,5.6))\n",
    "for  i in range(1,epochs+1):\n",
    "    gradient_m = -2*(np.sum(y) -m*np.sum(X) - n*b)\n",
    "    stepsize = learning_rate*gradient_m # step size = LearningRate * gradient\n",
    "    b = b - stepsize  # gradient decent formula.\n",
    "\n",
    "    ygds = m*X + b\n",
    "    # Only label the first and last epoch to keep legend clean\n",
    "    label = f'Epoch {i}' if i == 1 or i == epochs or  i%5 == 0  else None\n",
    "    plt.plot(X,ygds,'-.',label=label,alpha=0.2)\n",
    "\n",
    "plt.plot(X, y_pred,color='red',label='Linear Regression fit')\n",
    "plt.scatter(X,y, color='blue', label='Data Points')\n",
    "plt.grid()\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Gradient Descent Progression and Linear Regression (Red Line)')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "main_path  = Path(\"/home/mrafiku/AI_learning/machine-learning-basics\")\n",
    "results_path = main_path / \"results\"\n",
    "result_Linear_Regression_gdsc = results_path / \"Linear_Regression\"/\"usingGdsc\"\n",
    "result_Linear_Regression_gdsc.mkdir(parents=True, exist_ok=True)\n",
    "filename = f\"gdsc_progression_epochs_{epochs}_ALL.png\"\n",
    "plt.savefig(result_Linear_Regression_gdsc / filename)\n",
    "plt.show()\n",
    "plt.close(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "# The Below cell code is to convert the png images to .gif image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "from PIL import Image - # pip install pillow\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# 1. Define paths\n",
    "image_path = Path(\"/home/mrafiku/AI_learning/machine-learning-basics/results/Linear_Regression/usingGdsc\")\n",
    "gif_path = image_path / \"gdsc_animation.gif\"\n",
    "\n",
    "# 2. Grab all images and sort them numerically\n",
    "# It's important to sort them so the animation follows the epoch order\n",
    "images = sorted(list(image_path.glob(\"gdsc_progression_epochs_*.png\")), \n",
    "                key=lambda x: int(x.stem.split('_')[-1]))\n",
    "\n",
    "# 3. Load images into a list\n",
    "frames = [Image.open(img) for img in images]\n",
    "\n",
    "# 4. Save as GIF\n",
    "if frames:\n",
    "    frames[0].save(\n",
    "        gif_path,\n",
    "        format=\"GIF\",\n",
    "        append_images=frames[1:],\n",
    "        save_all=True,\n",
    "        duration=500, # 500ms per frame (half a second)\n",
    "        loop=0        # 0 means it loops forever\n",
    "    )\n",
    "    print(f\"GIF saved successfully at: {gif_path}\")\n",
    "else:\n",
    "    print(\"No images found to create GIF.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## This below is Mine code for the 'Batch gradient descent algorithm'.\n",
    "## This is compared with the SKLEARN Linear Regression and Graph of both is also compared.\n",
    "## carefull about the learning rate (alpha):\n",
    "-1 alpha should not be much less.\n",
    "\n",
    "-2 alpha should not be too high -> otherwise Never converge.\n",
    "\n",
    "-3 alpha should be appropraite. If data is normalize between [-1,1] then can choose alpha = 0.01 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_implement.architecture.gradient_dsc_mine import Mine_GradientDescent_LinaerRgression # this is mine gds LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Mine_GradientDescent_LinaerRgression:\n",
    "#     def __init__(self,learning_rate = None,epochs =None,initial_b =None, initial_w =None):\n",
    "#         self.learning_rate = learning_rate\n",
    "#         self.epochs = epochs\n",
    "#         self.b = initial_b\n",
    "#         self.w = initial_w\n",
    "#         self.J_w_b_history = {\"w\":[], \"b\":[], \"J_cost\":[]}\n",
    "#         # self.J_cost_hsitory = []\n",
    "\n",
    "#     def fit(self,X,y):\n",
    "#         X = np.asarray(X, dtype=float)\n",
    "#         y = np.asarray(y, dtype=float)\n",
    "#         X = X.reshape(-1, X.shape[1])  # Ensure X is 2D\n",
    "#         m,d = X.shape\n",
    "#         if self.w is None:\n",
    "#             self.w = np.zeros((d,1))\n",
    "#         # self.w = np.zeros((d,1))  # initializing weights to zero vector of shape (d,1)\n",
    "#         # print(f\"d : {d} - {self.w.shape} initial w: {self.w.flatten()} and shape of w: {self.w.shape} and initial b: {self.b}\")\n",
    "\n",
    "#         for epoch in range(self.epochs):\n",
    "            \n",
    "#             unitV = np.ones((m,1))    \n",
    "#             y = y.reshape(-1,1)\n",
    "#             # print(f\" shape of the unitV: {unitV.shape} and Transpose of unitV: {(unitV.T).shape}\")\n",
    "            \n",
    "#             y_hat = X@self.w + self.b*unitV  # prediction using the hypothesis, written in matrix form so, \n",
    "#             # it can work for single features as well as muultiple features. and cover the m no of training example as well as for single traing example.\n",
    "\n",
    "#             error = y_hat - y   # this term is common in both the gradient of b,w.[Xw + b*1 - y]-> [X(mxd)W(dx1) + b*1(1Xm unit vector) - y(mx1))]\n",
    "#             # self.J_cost_hsitory[\"J_cost\"].append(np.mean(error**2))  # cost function history for each epoch.\n",
    "#             # grad_b = np.mean(error) or 1/m*unitV.T @ error or 1/m*np.dot(unitV.T,error)\n",
    "#             # grad_b = 1/m*unitV.T@ (X@self.w + self.b*unitV - y)\n",
    "#             # grad_b = np.mean(error)\n",
    "#             # grad_b = 1/m*np.dot(unitV.T,error)\n",
    "#             grad_b =  1/m*unitV.T @ error\n",
    "#             self.b = self.b - self.learning_rate*grad_b\n",
    "\n",
    "#             # grad_w = 1/m*X.T @ (X@self.w +self.b*unitV - y)\n",
    "#             grad_w = 1/m*X.T@ error\n",
    "#             self.w = self.w - self.learning_rate*grad_w\n",
    "#             # print(f\"Epoch: {epoch+1}/{self.epochs},coefficient (w): {self.w.flatten()} intercept_b: {self.b} \")\n",
    "#             self.J_w_b_history[\"w\"].append(self.w.flatten())\n",
    "#             self.J_w_b_history[\"b\"].append(self.b)\n",
    "#             self.J_w_b_history[\"J_cost\"].append(np.mean(error**2))\n",
    "            \n",
    "#         return self.b,self.w,self.J_w_b_history \n",
    "    \n",
    "#     def predict(self,X_test):\n",
    "#         X_test = np.asarray(X_test, dtype=float)\n",
    "#         unitV1 = np.ones((X_test.shape[0],1))\n",
    "#         y_pred = unitV1*self.b + X_test@self.w\n",
    "        \n",
    "#         return y_pred\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_dir\n",
    "filename =  \"placement.csv\" # \"realest.csv\" #\"placement.csv\"\n",
    "filepath = \"/home/mrafiku/AI_learning/machine-learning-basics/data/Linear_Regression/placementdata\" # \"/home/mrafiku/AI_learning/machine-learning-basics/data/Linear_Regression/chicago_houseprice/\" \n",
    "filepath =  Path(filepath)    \n",
    "print(f\"filepath: {filepath}\")\n",
    "reader = DataReader(filepath, filename,df_want = True,split = True)\n",
    "df, X_train, X_test, y_train, y_test = reader.run() # df, X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test,y_test\n",
    "print(f\"X_test:{X_test.shape} type: {type(X_test)} and y_test : {y_test.shape}\")\n",
    "print(f\"X_train:{X_train.shape} and y_train : {y_train.shape}\")\n",
    "# y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "epochs = 1000\n",
    "MyGd = Mine_GradientDescent_LinaerRgression(learning_rate=0.001,epochs=epochs,initial_b=0, initial_w = None) \n",
    "MyGd.fit(X_train,y_train)\n",
    "intercept = MyGd.b\n",
    "coefficient = MyGd.w\n",
    "print(f\"Coefficient (w): {coefficient.flatten()} and Intercept (b): {intercept} \")\n",
    "\n",
    "w = MyGd.J_w_b_history[\"w\"]\n",
    "b = MyGd.J_w_b_history[\"b\"]\n",
    "j_cost = MyGd.J_w_b_history[\"J_cost\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_flat = np.array(MyGd.J_w_b_history[\"w\"]).flatten()\n",
    "b_flat = np.array(MyGd.J_w_b_history[\"b\"]).flatten()\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Create a \"Map\" (Grid) around your path\n",
    "# Find the min/max of your history to set the plot boundaries\n",
    "w_range = np.linspace(min(w_flat)-500, max(w_flat)+500, 100)\n",
    "b_range = np.linspace(min(b_flat)-500, max(b_flat)+500, 100)\n",
    "W, B = np.meshgrid(w_range, b_range)\n",
    "\n",
    "# 2. Calculate the Cost for the ENTIRE grid\n",
    "# You need your actual compute_cost function here\n",
    "def compute_cost(x, y, w, b):\n",
    "    m = len(x)\n",
    "    return (1/(2*m)) * np.sum((w * x + b - y)**2)\n",
    "\n",
    "# Vectorize the cost calculation for the grid\n",
    "Z = np.zeros(W.shape)\n",
    "for i in range(W.shape[0]):\n",
    "    for j in range(W.shape[1]):\n",
    "        Z[i,j] = compute_cost(X_train, y_train, W[i,j], B[i,j])\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "ax = fig.add_subplot(111, projection ='3d')\n",
    "ax.plot_surface(W,B,Z,cmap='viridis',alpha=0.8)\n",
    "ax.set_xlabel('w')\n",
    "ax.set_ylabel('b')\n",
    "ax.set_zlabel('Cost J(w,b)')\n",
    "ax.set_title('Cost Surface')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"Warning: NaN values found in cost grid. Check compute_cost function and input data.\")    \n",
    "# 3. Plot the \"Full Bowl\" (Concentric Circles)\n",
    "plt.figure(figsize=(8, 6))\n",
    "# Using plt.contour (lines) and plt.contourf (filled colors)\n",
    "plt.contourf(W, B, Z, levels=50, cmap='viridis', alpha=0.8)\n",
    "plt.contour(W, B, Z, levels=20, colors='white', alpha=0.2) # Adding line rings\n",
    "\n",
    "# 4. Overlay YOUR Gradient Descent path on the map\n",
    "plt.plot(w_flat, b_flat, color='magenta', marker='.', label='Your GD Path')\n",
    "plt.scatter(w_flat[-1], b_flat[-1], color='red', marker='x', s=100, zorder=5)\n",
    "\n",
    "plt.xlabel('w')\n",
    "plt.ylabel('b')\n",
    "plt.title('Contour Plot')\n",
    "plt.colorbar(label='Cost J(w,b)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "j_cost = MyGd.J_w_b_history[\"J_cost\"]\n",
    "# iterations = range(1,epochs+1)\n",
    "iterations = range(1,len(j_cost)+1)\n",
    "fig,ax =  plt.subplots(1,1)\n",
    "ax.plot(iterations, j_cost, '-.g')\n",
    "ax.set_xlabel('Iterations')\n",
    "ax.set_ylabel('Cost J(b,w)')\n",
    "ax.set_title('Cost Function History during Gradient Descent')\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# MyGd = Mine_GradientDescent_LinaerRgression(learning_rate=0.1,epochs=25,initial_b=0, initial_w = np.zeros((X.shape[1],1)))\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(f\"SKLEARN : --> model.coef_: {model.coef_} and model.intercept_: {model.intercept_}\")\n",
    "y_pred_gd = MyGd.predict(X_test)\n",
    "\n",
    "fig,ax = plt.subplots(1,2, figsize=(11,6.3))\n",
    "ax[0].plot(X_test,y_pred_gd,'-.y',label='Predicted - MyGdsc')\n",
    "ax[0].scatter(X_test,y_test, color='blue', label='Actual', alpha=0.6)\n",
    "ax[0].set_xlabel('CGPA')\n",
    "ax[0].set_ylabel('Package')\n",
    "ax[0].set_title('My Gradient Descent Linear Regression')\n",
    "ax[0].grid()\n",
    "ax[1].plot(X_test,y_pred,'-.r',label='Predicted - Sklearn')\n",
    "ax[1].scatter(X_test,y_test, color='blue', label='Actual', alpha=0.6)\n",
    "ax[1].set_xlabel('CGPA')\n",
    "ax[1].set_ylabel('Package')\n",
    "ax[1].set_title('My sklearn Regression')\n",
    "ax[1].grid()\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Coefficient (w): [0.54925495] and Intercept (b): -0.8101326877470646 \n",
    "# SKLEARN : --> model.coef_: [[0.57425647]] and model.intercept_: [-1.02700694]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X)\n",
    "y1 =X[[3]]\n",
    "y2 = X[3]\n",
    "print(f\"shape y1 :{y1.shape}, and shape of y2:{y2.shape}\")\n",
    "print(X[[3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "## Confusion in why Stochastic is faster than the Batch gradient descent method: \n",
    "-    In stochastic if we see in each epoch the loop it is iterating as number of time as Number of row in  X_tarin (number of training examples). \n",
    "-   But weight converge faster than the batch gradient, because in batch we get weight after all traing examples.\n",
    "-   raw number of mathematical operations per epoch, SGD actually involves slightly more overhead because you are updating the weights 1000(number of training examples, let'say 1000) times instead of once.\n",
    "-   However, \"Faster\" in Machine Learning doesn't mean \"the loop runs faster.\" It means \"it reaches the **minimum error in much less time.**\"\n",
    "1. **The Practical Example: 1000 Rows, 3 Features** \n",
    "    - Let’s use your dimensions: $X$ is $1000 \\times 3$.\n",
    "\n",
    "    **Batch Gradient Descent (The \"Wait for it...\" approach)** \n",
    "    - **The Math:** You calculate the prediction for all 1,000 rows. You calculate the error for all 1,000 rows. You sum them up.\n",
    "    - **The Update:** You move your weights once.The Problem: If you have 1,000,000 rows instead of 1,000, your computer might run out of RAM just trying to calculate that one single step. You are doing a massive amount of work for one tiny adjustment.\n",
    "    **Stochastic Gradient Descent (The \"Quick Learner\" approach)**\n",
    "    - **The Math:** You look at Row #1. You immediately see that the weight for Feature #1 is too high. \n",
    "\n",
    "    **The Update:** You move your weights immediately.\n",
    "\n",
    "    **The Speed Advantage:** By the time you have finished your first epoch (looped 1,000 times), you have adjusted your weights 1,000 times.The Result: Usually, SGD finds a \"good enough\" solution after only 1 or 2 epochs. Batch GD might need 100 epochs (100 full passes) to get to that same level of accuracy.\n",
    "\n",
    "    **Analogy**\n",
    "\n",
    "    - **Batch:** Imagine you are learning to throw a ball into a hoop.Batch: You throw 1,000 balls, someone records where they all landed, averages the distance, and then tells you: \"Next time, aim 2 inches higher.\n",
    "\n",
    "   - **Stochastic:**  You throw one ball. It goes low. You immediately adjust your aim for the second ball. By the 1,000th ball, you are already hitting the hoop.\n",
    "\n",
    "## 2. Step-by-Step Difference in Matrix Operations\n",
    "\n",
    "Let's look at what's happening to the weights **$\\mathbf{w}$** (a **$3 \\times 1$** vector) in one epoch.\n",
    "I am using the diabities dataset: which has 442 rows(training examples) and some features.\n",
    "| Step | Batch GD | Stochastic GD (SGD) | Mini-Batch GD |\n",
    "|------|----------|----------------------|---------------|\n",
    "| **1. Data Access** | Looks at all 442 rows at once. | Looks at 1 row at a time. | Looks at a **\"chunk\"** (e.g., 32 rows). |\n",
    "| **2. Gradient Calc** | Calculates the average slope of all 442 points. | Calculates the slope of just that 1 point. | Calculates the average slope of those 32 points. |\n",
    "| **3. Update** | Updates weights once per epoch. | Updates weights 442 times per epoch. | Updates weights 14 times (442/32) per epoch. |\n",
    "| **4. Efficiency** | Slow. Must wait for all math to finish. | Fast per step, but inefficient for CPUs. | Sweet Spot. Uses **\"Vectorization\"** to do math fast. |\n",
    "\n",
    "- this is another table:\n",
    "\n",
    "| Step | Batch Gradient Descent | Stochastic Gradient Descent |\n",
    "|------|-------------------------|-----------------------------|\n",
    "| **Input size** | $1000 \\times 3$ matrix | $1 \\times 3$ vector |\n",
    "| **Gradients** | $\\frac{1}{n}\\mathbf{X}^T(\\mathbf{X}\\mathbf{w}-\\mathbf{y})$ | $\\mathbf{x}_i^T(\\mathbf{x}_i\\mathbf{w}-y_i)$ |\n",
    "| **Update frequency** | Once per epoch | 1,000 times per epoch |\n",
    "| **Hardware** | Uses vectorization (**Fast**) | Heavy looping (**Slow**) |\n",
    "| **Convergence** | Takes many full passes to learn | Learns almost everything in the first pass |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = np.zeros((3,1))\n",
    "# w1.ndim\n",
    "w1.shape\n",
    "# Stochastic Gradien Batch Algorithm:\n",
    "X = np.random.randint(0,100,size=(5,2))\n",
    "print(f\"Original order of indices:\\n {X}\")\n",
    "for i in range(2):\n",
    "    # print(f\"Epoch {i+1}:\")\n",
    "    ind = np.random.permutation(5)\n",
    "    # print(f\"iteration {i}: {ind} and shape of ind: {ind.shape} and dimension:{ind.ndim} \")\n",
    "    X_suffle = X[ind]\n",
    "    # print(f\"Shuffled X:\\n {X_suffle}\\n and shape of x_suffle:{X_suffle.shape}\")\n",
    "x_i = X_suffle[2].reshape(-1,1)\n",
    "print(f\" before  x_i :{x_i.shape}\")\n",
    "# x_i.reshape(-1,1)\n",
    "# x_i.shape\n",
    "x_t = x_i.T\n",
    "print(f\"x_ after reshaping to 1 row: {x_i.T}\\n and \\n{x_t.shape}\")\n",
    "\n",
    "ysingle = 1\n",
    "ysingle.reshape(-1,1)\n",
    "print(f\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StocasticGD:\n",
    "    def __init__(self,learning_rate = None,epochs = None ):\n",
    "        self.lr = learning_rate if learning_rate is not None else 0.01\n",
    "        self.epochs = epochs if epochs is not None else 1000\n",
    "        self.w = None\n",
    "        self.b = 0\n",
    "        self.jcost = []\n",
    "        \n",
    "    def fit(self,X,y):\n",
    "        m,d = X.shape\n",
    "        if self.w is None:\n",
    "            # self.w = np.zeros(d)\n",
    "            self.w = np.zeros((d,1)) # matrix form.\n",
    "            print(f\"size of w just after initializing with np.zeros:-----------> {self.w.shape}\")\n",
    "            # jcost = []\n",
    "            unitV = np.ones((m,1)) \n",
    "    \n",
    "        # self.w = np.zeros((d,1))  # initializing weights to zero vector\n",
    "        for epoch in range(self.epochs):\n",
    "            # suffle the Xin ecah epoch.\n",
    "            indices  = np.random.permutation(m) # it will create the random number of sequence between 0(include) to m(not included).\n",
    "            # indices0_4 = np.random.permutation(5) # o/p like this--> [4 0 3 1 2] ecah time it will be different but b/w 0-5(not included).\n",
    "\n",
    "            X_suffle = X[indices] # X_suffle :--> rows of X is suffled randomly.with shape m,d\n",
    "            y_suffle = y[indices] # similarly y row also suffled randomly each epoch.\n",
    "            J_error =  [] # for each loop.\n",
    "            for i in range(m):\n",
    "\n",
    "                # Pick ONE sample (Vectorized as 1xFeatures: X_suufle[i] -> is vector of size 1xFeatures )\n",
    "                # x_i = X_suffle[i]\n",
    "                x_i = X_suffle[i].reshape(-1,1).T  # This is just for writing in matrix form.\n",
    "                # y_i = y_suffle[i]\n",
    "                y_i = y_suffle[i].reshape(1,1)   # This is also for just writing in the matrix form.\n",
    "\n",
    "                # 4. Predict (Dot product: Scalar result)\n",
    "                # y_hat = np.dot(x_i,self.w) + self.b\n",
    "\n",
    "                y_hat = x_i@self.w + self.b\n",
    "\n",
    "                # error is a scalar\n",
    "                error = y_hat - y_i #  Compute Gradients (Matrix Form)\n",
    "                squared_error = error**2\n",
    "                J_error.append(squared_error) # sqaure of error.\n",
    "                # dw = x_i*error # dw = x_i * error (Result is a vector of shape n_features)\n",
    "                dw = x_i.T * error # dw = x_i * error (Result is a vector of shape n_features)\n",
    "\n",
    "                db = error\n",
    "                # if (i == 0) and (epoch == 0 or epoch == self.epochs) or (i == m-1)and(epoch == 0 or epoch == self.epochs):\n",
    "                #     print(f\"x_i after reshaping an dtranspose:(expected:1x nfeatures) size::---------->{x_i.shape}\")\n",
    "                #     print(f\"y_i after reshaping an dtranspose:(expected:1x1) size::---------->type of {type(y_i)} and {y_i.shape}\")\n",
    "                #     print(f\"y_hat after reshaping an dtranspose:(expected:1x1) size::---------->type of {type(y_hat)} and {y_hat.shape}\")\n",
    "\n",
    "                #     print(f\"y_hat size::---------->{y_hat.shape}\")\n",
    "                #     print(f\"error size:----------> {error.shape}\")\n",
    "                #     print(f\"dw size:---------->{dw.shape}\")\n",
    "                #     print(f\"db size:---------->{db.shape} and its type: {type(db)}\")\n",
    "\n",
    "            # Update Parameters \n",
    "                self.w -= self.lr*dw\n",
    "                self.b -= self.lr*db\n",
    "            J_errorMean = np.mean(J_error)          \n",
    "            self.jcost.append(J_errorMean)\n",
    "            \n",
    "        return self.jcost\n",
    "\n",
    "    def predict(self,X):\n",
    "        # X --> X_test # Full Matrix Multiplication for prediction: Y = Xw + b\n",
    "        unitV1 = np.ones((X_test.shape[0],1))\n",
    "        # y_predict_dot_method=  np.dot(X, self.w) + self.b\n",
    "        \n",
    "        y = X@self.w + unitV1@self.b  \n",
    "        # y = X@self.w + self.b *unitV1\n",
    "        return y\n",
    "\n",
    "\n",
    "#  this is for the class.\n",
    "from sklearn.datasets import load_diabetes\n",
    "X,y = load_diabetes(return_X_y=True)\n",
    "print(f\"diabities data sets: {type(load_diabetes)} and X,y: {X.shape}and y :{y.shape}\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y)\n",
    "mystochastic_gd = StocasticGD(epochs=100)\n",
    "mystochastic_gd.fit(X_train,y_train)\n",
    "mystochastic_gd.predict(X_test)\n",
    "\n",
    "# Plotting the result\n",
    "num_epochs = mystochastic_gd.epochs\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(num_epochs), mystochastic_gd.jcost)\n",
    "plt.title(\"Cost (SGD) :large beacuse data is not normalize for best result Normalize data \")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Mean Squared Error\")\n",
    "plt.grid()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aftere scaling we see the cost .\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_diabetes\n",
    "X,y = load_diabetes(return_X_y=True)\n",
    "\n",
    "print(f\"diabities data sets: {type(load_diabetes)} and X,y: {X.shape}and y :{y.shape}\")\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y)\n",
    "#  scaling the train and test data:\n",
    "scaler = StandardScaler()  # Fit only on training data to avoid \"data leakage\"\n",
    "# Transform test data using the mean/std from training data\n",
    "X_train_scaled = scaler.fit_transform(X_train)  # for more deatils see the notes/.. docs.\n",
    "X_test_scaled = scaler.transform(X_test)  \n",
    "#  this the meansquare error WITHOUT normalization of the data. \n",
    "mystochastic_gd = StocasticGD(epochs=100)\n",
    "mystochastic_gd.fit(X_train,y_train)\n",
    "mystochastic_gd.predict(X_test)\n",
    "\n",
    "#  this the meansquare error with NORMALIZATION  of the data. \n",
    "mystochastic_gd_scaled = StocasticGD(epochs=100)\n",
    "mystochastic_gd_scaled.fit(X_train_scaled,y_train)\n",
    "mystochastic_gd_scaled.predict(X_test_scaled)\n",
    "\n",
    "# bOTH SCALED AND UNSCALED  the result Plotting\n",
    "fig,ax = plt.subplots(1,2)\n",
    "num_epochs = mystochastic_gd.epochs\n",
    "ax = ax.flatten()\n",
    "ax[0].plot(range(num_epochs), mystochastic_gd_scaled.jcost)\n",
    "ax[0].set_title(\"Cost (SGD) : scaled/Normalize data \")\n",
    "ax[0].set_xlabel(\"Epoch\")\n",
    "ax[0].set_ylabel(\"Mean Squared Error\")\n",
    "ax[0].grid()\n",
    "ax[1].plot(range(num_epochs), mystochastic_gd.jcost)\n",
    "ax[1].set_title(\"Cost (SGD) : Unscaled/Raw data\")\n",
    "ax[1].set_xlabel(\"Epoch\")\n",
    "ax[1].set_ylabel(\"Mean Squared Error\")\n",
    "ax[1].grid()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_diabetes\n",
    "\n",
    "diabetes = load_diabetes()\n",
    "X = diabetes.data\n",
    "y = diabetes.target\n",
    "feature_names = diabetes.feature_names\n",
    "\n",
    "# Create a grid of plots\n",
    "fig, axes = plt.subplots(2, 5, figsize=(20, 10))\n",
    "fig.suptitle(\"Diabetes Features vs. Disease Progression\", fontsize=16)\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.scatter(X[:, i], y, alpha=0.5, s=10, c='teal')\n",
    "    ax.set_xlabel(feature_names[i])\n",
    "    ax.set_ylabel(\"Target (y)\")\n",
    "    ax.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1. Prepare Data (2 features only for visualization)\n",
    "data = load_diabetes()  # or  X,y = load_diabetes(return_x_y = True) \n",
    "X = data.data[:, [2, 8]] # BMI and S5 index omly two X\n",
    "y = data.target.reshape(-1, 1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# 2. Modified SGD to track weight history\n",
    "def get_sgd_path(X, y, lr=0.1, epochs=3):\n",
    "    m, d = X.shape\n",
    "    w = np.zeros((d, 1))\n",
    "    b = 0\n",
    "    path = [] # To store (w1, w2)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for i in range(m):\n",
    "            # Save current weights before update\n",
    "            path.append(w.copy())\n",
    "            if i == 0:\n",
    "                print(f\"path w copy:{path} and {len(path)} and shape of ecah element of path:{path[i].shape}\")\n",
    "\n",
    "            x_i = X[i:i+1]\n",
    "            y_i = y[i:i+1]\n",
    "            \n",
    "            # Predict and update\n",
    "            y_hat = x_i @ w + b\n",
    "            error = y_hat - y_i\n",
    "            dw = x_i.T * error\n",
    "            \n",
    "            w -= lr * dw\n",
    "            b -= lr * error.item()\n",
    "            \n",
    "    return np.array(path).reshape(-1, 2)\n",
    "\n",
    "# 3. Function to plot the Contour and Path\n",
    "def plot_contour_path(X, y, path):\n",
    "    # Create a grid of weight values\n",
    "    w1_range = np.linspace(-100, 100, 100)\n",
    "    w2_range = np.linspace(-100, 100, 100)\n",
    "    W1, W2 = np.meshgrid(w1_range, w2_range)\n",
    "    \n",
    "    # Calculate Cost (MSE) for every point on the grid\n",
    "    Z = np.zeros(W1.shape)\n",
    "    for i in range(W1.shape[0]):\n",
    "        for j in range(W1.shape[1]):\n",
    "            w_temp = np.array([[W1[i,j]], [W2[i,j]]])\n",
    "            y_pred = X @ w_temp # Ignoring bias for simple visualization\n",
    "            Z[i,j] = np.mean((y_pred - y)**2)\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.contour(W1, W2, Z, levels=20, cmap='viridis')\n",
    "    plt.plot(path[:, 0], path[:, 1], color='red', marker='o', \n",
    "             markersize=5, linewidth=1, label='SGD Path (Zig-Zag)')\n",
    "    \n",
    "    plt.title(\"SGD Convergence Path on Cost Contour\")\n",
    "    plt.xlabel(\"Weight 1 (BMI)\")\n",
    "    plt.ylabel(\"Weight 2 (S5)\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Run it\n",
    "weight_path = get_sgd_path(X, y, lr=0.01, epochs=1)\n",
    "plot_contour_path(X, y, weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
