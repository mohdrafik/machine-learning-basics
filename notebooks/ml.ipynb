{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# linear Regression Example and implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from pathlib import Path\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = Path.cwd().parent \n",
    "base_dir = Path(base_dir)\n",
    "print(f\"Base directory: {base_dir}\")\n",
    "from ml_implement.general_utils.data_read import DataReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filename = 'realest.csv'\n",
    "filepath = base_dir/'data'/'Linear_Regression'/'chicago_houseprice'\n",
    "# reader = DataReader(filepath, filename)\n",
    "# reader = DataReader(filepath=filepath, filename=filename, split=True, df_want=True, target_column=\"Price\")\n",
    "reader = DataReader(filepath=filepath, filename=filename, split=True, df_want=True, target_column=None)\n",
    "res = reader.run() # result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile = r\"/home/mrafiku/AI_learning/machine-learning-basics/data/Linear_Rigression/placementdata/placement.csv\"\n",
    "df = pd.read_csv(datafile)\n",
    "print(df.head())\n",
    "# print(df.info())\n",
    "# print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CSV files often include:\n",
    "# Leading/trailing spaces\n",
    "# Hidden characters\n",
    "# -  Uppercase/lowercase mismatches\n",
    "# -  UTF-8 BOM characters\n",
    "# -  Inconsistent headers\n",
    "# Even if the DataFrame looks correct, the internal label might be different.\n",
    "\n",
    "print(df.columns.tolist())\n",
    "# print(df.columns)\n",
    "# Fix: normalize column names\n",
    "df.columns = df.columns.str.strip().str.lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df['cgpa'],df['package'])\n",
    "plt.xlabel('CGPA')\n",
    "plt.ylabel('Package')\n",
    "plt.title('CGPA vs package')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape # (n_rows, n_columns)\n",
    "df.size  # total number of elements in the DataFrame\n",
    "df.ndim  # number of dimensions (axes) of the DataFrame\n",
    "print(f\"DataFrame Shape: {df.shape} | Size: {df.size} | Dimensions: {df.ndim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = df[\"cgpa\"]\n",
    "# X.shape\n",
    "# X.ndim\n",
    "# X = np.array(X).reshape(-1,1)\n",
    "# # X = X.reshape(-1,1)\n",
    "# X.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,0].values\n",
    "y = df.iloc[:,1].values \n",
    "print(f\"X shape: {X.shape} | type ox X: {type(X)}| y shape: {y.shape} | type of y: {type(y)}\")\n",
    "\n",
    "if X.ndim == 1:\n",
    "    X = X.reshape(-1,1)\n",
    "if y.ndim == 1:\n",
    "    y = y.reshape(-1,1)\n",
    "print(f\"After Reshaping: X shape: {X.shape}, y shape: {y.shape} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "- df.describe()\n",
    "- df.columns.tolist()\n",
    "- print(f\" {X} \\n {y}\")\n",
    "- print(f\"X shape: {X.shape}, y shape: {y.shape} and \\n X type: {type(X)} | y type: {type(y)} \")  \n",
    "#--> These are the output of the print statement \n",
    "- those clearly shows: That shapes of X,y is one D array, so for the Sklearn we need to reshape it to 2D array : for fit and predict methods. \n",
    "- X shape: (200,), y shape: (200,) and \n",
    "- X type: <class 'numpy.ndarray'> | y type: <class 'numpy.ndarray'>\n",
    "#--> Reshaping the X to 2D array in th ebelow next cell: ---> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \" this is function to fit Linear Regression model and return X_train,X_test,y_train, y_test,model\"\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def LR_modelfit_predict_Result(X, y,test_size = None):\n",
    "    \"\"\"\n",
    "    Docstring for fit_LR_model_predict_Result is return result: X_train,X_test,y_train, y_test,model\n",
    "    :param X: X is numpy array of shape (n_samples, n_features)\n",
    "    :param y: y is numpy array of shape (n_samples, )\n",
    "    :param test_size: test_size is None,bydefault is 0.2 if None, 0 if 0, else float value between 0 and 1\n",
    "    :return: X_train,X_test,y_train, y_test,model\n",
    "    \"\"\"\n",
    "    if X.ndim == 1:\n",
    "        X = X.reshape(-1,1)\n",
    "    if y.ndim == 1:\n",
    "        y = y.reshape(-1,1)\n",
    "    if test_size == 0:\n",
    "        print(\"using all data for training as test_size is 0\")\n",
    "        X_train, y_train = X,y\n",
    "    if test_size is None:\n",
    "        test_size = 0.2\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "    if test_size is not None and test_size !=0:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "\n",
    "    print(f\"X_train shape: {X_train.shape}, X_test shape: {X_test.shape} , y_train shape: {y_train.shape}, y_test shape: {y_test.shape}\")\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    # y_pred = model.predict(X_test)\n",
    "    return X_train,X_test,y_train, y_test,model\n",
    "    # print(f\"input_CGPA: {X_test[0,0]}, PREDICTED_PACKAGE: {y_pred[0,0]} and ACTUAL PACKAGE: {y_test[0,0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this a module to import the metric of the linear regression from scratch\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score\n",
    "X_train,X_test,y_train, y_test,model = LR_modelfit_predict_Result(X, y,test_size=0.2)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test,y_pred)\n",
    "mse = mean_squared_error(y_test,y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test,y_pred)\n",
    "print(f\"MAE: {mae}, MSE: {mse}, RMSE: {rmse}, R2_score: {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn.datasets..make_regression() --> used to generate the regression datasets provided by the scikit-learn.\n",
    "\n",
    "from sklearn.datasets import make_regression\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "X,y =  make_regression(n_samples = 100,n_features=2,n_informative=2,n_targets=1,noise=0.1, random_state=42)\n",
    "# print(f\"{X} \\n {y}\")\n",
    "x1 = X[:,0]\n",
    "x2 = X[:,1]\n",
    "# plt.scatter(x1,x2,y)\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "sc = ax.scatter(x1, x2, y, c=y, cmap='viridis', s=40, alpha=0.8)\n",
    "\n",
    "\n",
    "\n",
    "ax.set_xlabel('x1')\n",
    "ax.set_ylabel('x2')\n",
    "ax.set_zlabel('y')\n",
    "ax.set_title('3D scatter: x1, x2 vs y')\n",
    "fig.colorbar(sc, ax=ax, shrink=0.5, label='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code for linear regresssion model from the basics without using sklearn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionMine():\n",
    "    def __init__(self):\n",
    "\n",
    "        pass\n",
    "    def fit(self,X,y):\n",
    "        n = X.shape[0]  # number of training examples.\n",
    "        d = X.shape[1]  # numbers of features\n",
    "        \n",
    "        X = np.hstack((np.ones((n,1)),X))  # adding bias term to the feature matrix. \n",
    "        A = (X.T)@X\n",
    "        A_inv = np.linalg.inv(A)\n",
    "        B = (X.T)@y\n",
    "        Theta = A_inv @ B   # parameters of the model.\n",
    "        self.theta = Theta\n",
    "        return Theta\n",
    "    \n",
    "    def predict(self,X):\n",
    "        n = X.shape[0]\n",
    "        X = np.hstack((np.ones((n,1)),X))  # adding bias term to the feature matrix. \n",
    "        y_pred = X @ self.theta\n",
    "        self.y_pred = y_pred\n",
    "        return y_pred\n",
    "    \n",
    "    def MAE(self,y_true,y_pred):\n",
    "        n_test = y_true.shape[0]\n",
    "        mae = np.abs(np.sum(y_true - y_pred))/n_test\n",
    "        return mae\n",
    "    \n",
    "    def MSE(self,y_true,y_pred):\n",
    "        n_test = y_true.shape[0]\n",
    "        mse = np.sum((y_true - y_pred)**2)/n_test\n",
    "        return mse\n",
    "    def RMSE(self,y_true,y_pred):\n",
    "        mse = self.MSE(y_true,y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        return rmse\n",
    "    def R2_score(self,y_true,y_pred):\n",
    "        ss_total = np.sum((y_true - np.mean(y_true))**2)\n",
    "        ss_residual = np.sum((y_true - y_pred)**2)\n",
    "        r2 = 1 - (ss_residual/ss_total)\n",
    "        return r2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# X_train,X_test,y_train,y_test= train_test_split(X,y,test_size =0.2,random_state = 42)\n",
    "# print(f\"X_train shape: {X_train.shape}, X_test shape: {X_test.shape} , y_train shape: {y_train.shape}, y_test shape: {y_test.shape}\")\n",
    "mlr = LinearRegressionMine()\n",
    "mlr.fit(X_train,y_train)\n",
    "y1_pred = mlr.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"input_CGPA: {X_test[0,0]}, PREDICTED_PACKAGE: {y1_pred[0,0]} and ACTUAL PACKAGE: {y_test[0,0]}\")\n",
    "# input_CGPA: 6.63, PREDICTED_PACKAGE: 2.7803134765595168 and ACTUAL PACKAGE: 2.79\n",
    "ame = mlr.MAE(y_test,y1_pred)\n",
    "mse = mlr.MSE(y_test,y1_pred)\n",
    "rmse = mlr.RMSE(y_test,y1_pred)\n",
    "r2 = mlr.R2_score(y_test,y1_pred)   \n",
    "print(f\"AME: {ame}, MSE: {mse}, RMSE: {rmse}, R2_score: {r2}\")\n",
    "# MAE: 0.23150985393278373, MSE: 0.08417638361329656, RMSE: 0.2901316659954521, R2_score: 0.7730984312051673\n",
    "\n",
    "# here we can see the results are comparable with the sklearn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {
    "keep_output": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Initialize the figure and axes\n",
    "fig, ax = plt.subplots(1, 2, figsize=(8, 4))  # 1 row, 2 columns\n",
    "\n",
    "# --- FIRST SUBPLOT (Index 0) ---\n",
    "ax[0].scatter(X_test, y_test, color='blue', label='Actual', alpha=0.6)\n",
    "ax[0].plot(X_test, y_pred, '-.r', label='Predicted - Sklearn')\n",
    "ax[0].set_xlabel('CGPA')\n",
    "ax[0].set_ylabel('Package')\n",
    "ax[0].set_title('Sklearn Model')\n",
    "ax[0].grid(True)  # Use ax[0].grid() instead of plt.grid()\n",
    "ax[0].legend()\n",
    "\n",
    "# --- SECOND SUBPLOT (Index 1) ---\n",
    "ax[1].scatter(X_test, y_test, color='blue', label='Actual', alpha=0.6)\n",
    "ax[1].plot(X_test, y1_pred, '-.y', label='Predicted - MineLR')\n",
    "ax[1].set_xlabel('CGPA')\n",
    "ax[1].set_ylabel('Package')\n",
    "ax[1].set_title('My_LinearRegression_Code Model')\n",
    "ax[1].grid(True)  # Use ax[1].grid() instead of plt.grid()\n",
    "ax[1].legend()\n",
    "\n",
    "# Final adjustments\n",
    "plt.tight_layout() \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent: \n",
    "# bn+1  = bn - α * ∇J / @ b = bn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "X,y  = make_regression(n_samples=7, n_features=1, n_targets=1, noise=50, random_state=42)\n",
    "# from sklearn.Linear_model import LinearRegression\n",
    "\n",
    "model  = LinearRegression()\n",
    "model.fit(X,y)\n",
    "y_pred = model.predict(X)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.scatter(X,y, color='blue', label='Data Points')\n",
    "plt.plot(X, y_pred, color='red', label='Linear Regression Line')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Linear Regression Fit')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "# model.intercept_  # b0  --> bias term\n",
    "model.coef_       # b1  --> weight term\n",
    "m = model.coef_[0]  # slope (m)\n",
    "print(f\"Slope (m): {m} and shape : {model.coef_.shape} \")\n",
    " \n",
    "learning_rate = 0.01  # learning rate\n",
    "n = X.shape[0]  # number of training examples\n",
    "b = 0\n",
    "epochs = 25\n",
    "        \n",
    "plt.figure(figsize=(8,5.6))\n",
    "for  i in range(1,epochs+1):\n",
    "    gradient_m = -2*(np.sum(y) -m*np.sum(X) - n*b)\n",
    "    stepsize = learning_rate*gradient_m # step size = LearningRate * gradient\n",
    "    b = b - stepsize  # gradient decent formula.\n",
    "\n",
    "    ygds = m*X + b\n",
    "    # Only label the first and last epoch to keep legend clean\n",
    "    label = f'Epoch {i}' if i == 1 or i == epochs or  i%5 == 0  else None\n",
    "    plt.plot(X,ygds,'-.',label=label,alpha=0.2)\n",
    "\n",
    "plt.plot(X, y_pred,color='red',label='Linear Regression fit')\n",
    "plt.scatter(X,y, color='blue', label='Data Points')\n",
    "plt.grid()\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Gradient Descent Progression and Linear Regression (Red Line)')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "\n",
    "main_path  = Path(\"/home/mrafiku/AI_learning/machine-learning-basics\")\n",
    "results_path = main_path / \"results\"\n",
    "result_Linear_Regression_gdsc = results_path / \"Linear_Regression\"/\"usingGdsc\"\n",
    "result_Linear_Regression_gdsc.mkdir(parents=True, exist_ok=True)\n",
    "filename = f\"gdsc_progression_epochs_{epochs}_ALL.png\"\n",
    "plt.savefig(result_Linear_Regression_gdsc / filename)\n",
    "plt.show()\n",
    "plt.close(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "# The Below cell code is to convert the png images to .gif image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "from PIL import Image - # pip install pillow\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# 1. Define paths\n",
    "image_path = Path(\"/home/mrafiku/AI_learning/machine-learning-basics/results/Linear_Regression/usingGdsc\")\n",
    "gif_path = image_path / \"gdsc_animation.gif\"\n",
    "\n",
    "# 2. Grab all images and sort them numerically\n",
    "# It's important to sort them so the animation follows the epoch order\n",
    "images = sorted(list(image_path.glob(\"gdsc_progression_epochs_*.png\")), \n",
    "                key=lambda x: int(x.stem.split('_')[-1]))\n",
    "\n",
    "# 3. Load images into a list\n",
    "frames = [Image.open(img) for img in images]\n",
    "\n",
    "# 4. Save as GIF\n",
    "if frames:\n",
    "    frames[0].save(\n",
    "        gif_path,\n",
    "        format=\"GIF\",\n",
    "        append_images=frames[1:],\n",
    "        save_all=True,\n",
    "        duration=500, # 500ms per frame (half a second)\n",
    "        loop=0        # 0 means it loops forever\n",
    "    )\n",
    "    print(f\"GIF saved successfully at: {gif_path}\")\n",
    "else:\n",
    "    print(\"No images found to create GIF.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## This below is Mine code for the 'Batch gradient descent algorithm'.\n",
    "## This is compared with the SKLEARN Linear Regression and Graph of both is also compared.\n",
    "## carefull about the learning rate (alpha):\n",
    "-1 alpha should not be much less.\n",
    "\n",
    "-2 alpha should not be too high -> otherwise Never converge.\n",
    "\n",
    "-3 alpha should be appropraite. If data is normalize between [-1,1] then can choose alpha = 0.01 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_implement.architecture.gradient_dsc_mine import Mine_GradientDescent_LinaerRgression # this is mine gds LR\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Mine_GradientDescent_LinaerRgression:\n",
    "#     def __init__(self,learning_rate = None,epochs =None,initial_b =None, initial_w =None):\n",
    "#         self.learning_rate = learning_rate\n",
    "#         self.epochs = epochs\n",
    "#         self.b = initial_b\n",
    "#         self.w = initial_w\n",
    "#         self.J_w_b_history = {\"w\":[], \"b\":[], \"J_cost\":[]}\n",
    "#         # self.J_cost_hsitory = []\n",
    "\n",
    "#     def fit(self,X,y):\n",
    "#         X = np.asarray(X, dtype=float)\n",
    "#         y = np.asarray(y, dtype=float)\n",
    "#         X = X.reshape(-1, X.shape[1])  # Ensure X is 2D\n",
    "#         m,d = X.shape\n",
    "#         if self.w is None:\n",
    "#             self.w = np.zeros((d,1))\n",
    "#         # self.w = np.zeros((d,1))  # initializing weights to zero vector of shape (d,1)\n",
    "#         # print(f\"d : {d} - {self.w.shape} initial w: {self.w.flatten()} and shape of w: {self.w.shape} and initial b: {self.b}\")\n",
    "\n",
    "#         for epoch in range(self.epochs):\n",
    "            \n",
    "#             unitV = np.ones((m,1))    \n",
    "#             y = y.reshape(-1,1)\n",
    "#             # print(f\" shape of the unitV: {unitV.shape} and Transpose of unitV: {(unitV.T).shape}\")\n",
    "            \n",
    "#             y_hat = X@self.w + self.b*unitV  # prediction using the hypothesis, written in matrix form so, \n",
    "#             # it can work for single features as well as muultiple features. and cover the m no of training example as well as for single traing example.\n",
    "\n",
    "#             error = y_hat - y   # this term is common in both the gradient of b,w.[Xw + b*1 - y]-> [X(mxd)W(dx1) + b*1(1Xm unit vector) - y(mx1))]\n",
    "#             # self.J_cost_hsitory[\"J_cost\"].append(np.mean(error**2))  # cost function history for each epoch.\n",
    "#             # grad_b = np.mean(error) or 1/m*unitV.T @ error or 1/m*np.dot(unitV.T,error)\n",
    "#             # grad_b = 1/m*unitV.T@ (X@self.w + self.b*unitV - y)\n",
    "#             # grad_b = np.mean(error)\n",
    "#             # grad_b = 1/m*np.dot(unitV.T,error)\n",
    "#             grad_b =  1/m*unitV.T @ error\n",
    "#             self.b = self.b - self.learning_rate*grad_b\n",
    "\n",
    "#             # grad_w = 1/m*X.T @ (X@self.w +self.b*unitV - y)\n",
    "#             grad_w = 1/m*X.T@ error\n",
    "#             self.w = self.w - self.learning_rate*grad_w\n",
    "#             # print(f\"Epoch: {epoch+1}/{self.epochs},coefficient (w): {self.w.flatten()} intercept_b: {self.b} \")\n",
    "#             self.J_w_b_history[\"w\"].append(self.w.flatten())\n",
    "#             self.J_w_b_history[\"b\"].append(self.b)\n",
    "#             self.J_w_b_history[\"J_cost\"].append(np.mean(error**2))\n",
    "            \n",
    "#         return self.b,self.w,self.J_w_b_history \n",
    "    \n",
    "#     def predict(self,X_test):\n",
    "#         X_test = np.asarray(X_test, dtype=float)\n",
    "#         unitV1 = np.ones((X_test.shape[0],1))\n",
    "#         y_pred = unitV1*self.b + X_test@self.w\n",
    "        \n",
    "#         return y_pred\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_dir\n",
    "filename = \"placement.csv\"\n",
    "filepath = \"/home/mrafiku/AI_learning/machine-learning-basics/data/Linear_Regression/placementdata\"\n",
    "filepath =  Path(filepath)    \n",
    "print(f\"filepath: {filepath}\")\n",
    "reader = DataReader(filepath, filename,df_want = True,split = True)\n",
    "df, X_train, X_test, y_train, y_test = reader.run() # df, X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test,y_test\n",
    "print(f\"X_test:{X_test.shape} type: {type(X_test)} and y_test : {y_test.shape}\")\n",
    "print(f\"X_train:{X_train.shape} and y_train : {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "epochs = 1000\n",
    "MyGd = Mine_GradientDescent_LinaerRgression(learning_rate=0.001,epochs=epochs,initial_b=0, initial_w = None) \n",
    "MyGd.fit(X_train,y_train)\n",
    "intercept = MyGd.b\n",
    "coefficient = MyGd.w\n",
    "print(f\"Coefficient (w): {coefficient.flatten()} and Intercept (b): {intercept} \")\n",
    "\n",
    "w = MyGd.J_w_b_history[\"w\"]\n",
    "b = MyGd.J_w_b_history[\"b\"]\n",
    "j_cost = MyGd.J_w_b_history[\"J_cost\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_flat = np.array(MyGd.J_w_b_history[\"w\"]).flatten()\n",
    "b_flat = np.array(MyGd.J_w_b_history[\"b\"]).flatten()\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Create a \"Map\" (Grid) around your path\n",
    "# Find the min/max of your history to set the plot boundaries\n",
    "w_range = np.linspace(min(w_flat)-2, max(w_flat)+2, 100)\n",
    "b_range = np.linspace(min(b_flat)-2, max(b_flat)+2, 100)\n",
    "W, B = np.meshgrid(w_range, b_range)\n",
    "\n",
    "# 2. Calculate the Cost for the ENTIRE grid\n",
    "# You need your actual compute_cost function here\n",
    "def compute_cost(x, y, w, b):\n",
    "    m = len(x)\n",
    "    return (1/(2*m)) * np.sum((w * x + b - y)**2)\n",
    "\n",
    "# Vectorize the cost calculation for the grid\n",
    "Z = np.zeros(W.shape)\n",
    "for i in range(W.shape[0]):\n",
    "    for j in range(W.shape[1]):\n",
    "        Z[i,j] = compute_cost(X_train, y_train, W[i,j], B[i,j])\n",
    "\n",
    "# 3. Plot the \"Full Bowl\" (Concentric Circles)\n",
    "plt.figure(figsize=(8, 6))\n",
    "# Using plt.contour (lines) and plt.contourf (filled colors)\n",
    "plt.contourf(W, B, Z, levels=50, cmap='viridis', alpha=0.8)\n",
    "plt.contour(W, B, Z, levels=20, colors='white', alpha=0.2) # Adding line rings\n",
    "\n",
    "# 4. Overlay YOUR Gradient Descent path on the map\n",
    "plt.plot(w_flat, b_flat, color='magenta', marker='.', label='Your GD Path')\n",
    "plt.scatter(w_flat[-1], b_flat[-1], color='red', marker='x', s=100, zorder=5)\n",
    "\n",
    "plt.xlabel('w')\n",
    "plt.ylabel('b')\n",
    "plt.title('Contour Plot')\n",
    "plt.colorbar(label='Cost J(w,b)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "j_cost = MyGd.J_w_b_history[\"J_cost\"]\n",
    "# iterations = range(1,epochs+1)\n",
    "iterations = range(1,len(j_cost)+1)\n",
    "fig,ax =  plt.subplots(1,1)\n",
    "ax.plot(iterations, j_cost, '-.g')\n",
    "ax.set_xlabel('Iterations')\n",
    "ax.set_ylabel('Cost J(b,w)')\n",
    "ax.set_title('Cost Function History during Gradient Descent')\n",
    "ax.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# MyGd = Mine_GradientDescent_LinaerRgression(learning_rate=0.1,epochs=25,initial_b=0, initial_w = np.zeros((X.shape[1],1)))\n",
    "from sklearn.linear_model import LinearRegression\n",
    "model = LinearRegression()\n",
    "model.fit(X_train,y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(f\"SKLEARN : --> model.coef_: {model.coef_} and model.intercept_: {model.intercept_}\")\n",
    "y_pred_gd = MyGd.predict(X_test)\n",
    "\n",
    "fig,ax = plt.subplots(1,2, figsize=(11,6.3))\n",
    "ax[0].plot(X_test,y_pred_gd,'-.y',label='Predicted - MyGdsc')\n",
    "ax[0].scatter(X_test,y_test, color='blue', label='Actual', alpha=0.6)\n",
    "ax[0].set_xlabel('CGPA')\n",
    "ax[0].set_ylabel('Package')\n",
    "ax[0].set_title('My Gradient Descent Linear Regression')\n",
    "ax[0].grid()\n",
    "ax[1].plot(X_test,y_pred,'-.r',label='Predicted - Sklearn')\n",
    "ax[1].scatter(X_test,y_test, color='blue', label='Actual', alpha=0.6)\n",
    "ax[1].set_xlabel('CGPA')\n",
    "ax[1].set_ylabel('Package')\n",
    "ax[1].set_title('My sklearn Regression')\n",
    "ax[1].grid()\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Coefficient (w): [0.54925495] and Intercept (b): -0.8101326877470646 \n",
    "# SKLEARN : --> model.coef_: [[0.57425647]] and model.intercept_: [-1.02700694]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
